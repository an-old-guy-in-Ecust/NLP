{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   id                                               text  \\\n0   0  共享一个额度，没啥必要，四个卡不要年费吗？你这种人头，银行最喜欢，广发是出了名的风控严，套现...   \n1   1                                 炸了，就2000.浦发没那么好心，草   \n2   2                               挂了电话自己打过去分期提额可以少分一点的   \n3   3               比如你首卡10k，二卡也10k，信报上显示邮政总共给你的授信额度是20k   \n4   4                                       3000吗，浦发总是这样   \n\n                                   testing_data_text  \n0  [共, 享, 一, 个, 额, 度, ，, 没, 啥, 必, 要, ，, 四, 个, 卡, ...  \n1  [炸, 了, ，, 就, 2, 0, 0, 0, ., 浦, 发, 没, 那, 么, 好, ...  \n2  [挂, 了, 电, 话, 自, 己, 打, 过, 去, 分, 期, 提, 额, 可, 以, ...  \n3  [比, 如, 你, 首, 卡, 1, 0, k, ，, 二, 卡, 也, 1, 0, k, ...  \n4               [3, 0, 0, 0, 吗, ，, 浦, 发, 总, 是, 这, 样]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>testing_data_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>共享一个额度，没啥必要，四个卡不要年费吗？你这种人头，银行最喜欢，广发是出了名的风控严，套现...</td>\n      <td>[共, 享, 一, 个, 额, 度, ，, 没, 啥, 必, 要, ，, 四, 个, 卡, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>炸了，就2000.浦发没那么好心，草</td>\n      <td>[炸, 了, ，, 就, 2, 0, 0, 0, ., 浦, 发, 没, 那, 么, 好, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>挂了电话自己打过去分期提额可以少分一点的</td>\n      <td>[挂, 了, 电, 话, 自, 己, 打, 过, 去, 分, 期, 提, 额, 可, 以, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>比如你首卡10k，二卡也10k，信报上显示邮政总共给你的授信额度是20k</td>\n      <td>[比, 如, 你, 首, 卡, 1, 0, k, ，, 二, 卡, 也, 1, 0, k, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3000吗，浦发总是这样</td>\n      <td>[3, 0, 0, 0, 吗, ，, 浦, 发, 总, 是, 这, 样]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 加载训练数据和测试数据\n",
    "train_dataframe = pd.read_csv('../data_and_baseline/train_data_public.csv')\n",
    "test_dataframe = pd.read_csv('../data_and_baseline/test_public.csv')\n",
    "\n",
    "# 将sentence-level信息切分为character-level的信息\n",
    "train_dataframe['BIO_anno'] = train_dataframe['BIO_anno'].apply(lambda x: x.split(' '))  # label\n",
    "train_dataframe['training_data_text'] = train_dataframe.apply(lambda row: list(row['text']), axis=1)\n",
    "test_dataframe['testing_data_text'] = test_dataframe.apply(lambda row: list(row['text']), axis=1)\n",
    "test_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "training_data_text_list = []\n",
    "testing_data_text_list = []\n",
    "for i in range(len(train_dataframe)):\n",
    "    training_data_text_list.append(train_dataframe.iloc[i]['training_data_text'])\n",
    "for i in range(len(test_dataframe)):\n",
    "    testing_data_text_list.append(test_dataframe.iloc[i]['testing_data_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='hfl/chinese-roberta-wwm-ext', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "access_token = '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'\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext', use_auth_token=access_token)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class SA_dataloader(DataLoader):\n",
    "    def __init__(self, dataset: TensorDataset, random: bool):\n",
    "        if random:\n",
    "            self.sampler = RandomSampler(dataset)\n",
    "        else:\n",
    "            self.sampler = SequentialSampler(dataset)\n",
    "        super().__init__(dataset, sa_config.batch_size, sampler=self.sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from SA_config import *\n",
    "\n",
    "\n",
    "# 将每一句转成数字（大于510做截断，小于510做PADDING，加上首尾两个标识，长度总共等于512）\n",
    "def convert_text_to_token(tokenizer, sentence, limit_size=510):\n",
    "    tokens = tokenizer.encode(sentence[:limit_size])  # 直接截断\n",
    "    # 补齐（pad的索引号就是0）\n",
    "    if len(tokens) < limit_size + 2:\n",
    "        tokens.extend([0] * (limit_size + 2 - len(tokens)))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 建立mask\n",
    "def attention_masks(input_tokens):\n",
    "    atten_masks = []\n",
    "    for seq in input_tokens:\n",
    "        # 如果有编码（>0）即为1, pad为0\n",
    "        seq_mask = [float(x > 0) for x in seq]\n",
    "        atten_masks.append(seq_mask)\n",
    "    return torch.tensor(atten_masks, dtype=torch.bool)\n",
    "\n",
    "\n",
    "# 对每个句子进行编码\n",
    "def input_tokens(tokenizer, data_text_list):\n",
    "    tokens = torch.tensor([convert_text_to_token(tokenizer, x, sa_config.max_position_embeddings - 2) for x in\n",
    "                           data_text_list])\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# train_dataloader: tokens_id,attention-masks,classess\n",
    "train_data_class = torch.tensor(train_dataframe['class'].values)\n",
    "train_data_sentence_list = list(train_dataframe['text'].values)\n",
    "test_data_sentence_list = list(test_dataframe['text'].values)\n",
    "test_input_tokens = input_tokens(tokenizer, test_data_sentence_list)\n",
    "train_input_tokens = input_tokens(tokenizer, train_data_sentence_list)\n",
    "train_attention_tokens = attention_masks(train_input_tokens)\n",
    "test_attention_tokens = attention_masks(test_input_tokens)\n",
    "test_dataset = TensorDataset(test_input_tokens, test_attention_tokens)\n",
    "test_dataloader = SA_dataloader(test_dataset, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "\n",
    "class Roberta_FocalLoss(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.roberta = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext', config=config)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=config.hidden_size,\n",
    "                                               num_heads=config.num_heads, batch_first=True)\n",
    "        self.GRU = nn.GRU(input_size=config.hidden_size, hidden_size=config.hidden_size // 2, bidirectional=True)\n",
    "        self.classifier = nn.Linear(in_features=config.hidden_size * 2, out_features=config.num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, classes=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output, pooler_output = outputs[0], outputs[1]\n",
    "        attention_output, _ = self.attention(output, output, output)\n",
    "        gru_output, _ = self.GRU(attention_output)\n",
    "        concat_output = torch.cat((gru_output[:, -1, :], pooler_output), dim=1)\n",
    "        logits = self.classifier(concat_output)\n",
    "        logits = self.softmax(logits)\n",
    "        output = (logits,)\n",
    "        if classes is not None:\n",
    "            classes = classes.reshape(-1, 1)\n",
    "            one_hot = torch.zeros(sa_config.batch_size, sa_config.num_classes, device=sa_config.device).scatter_(1,\n",
    "                                                                                                                 classes,\n",
    "                                                                                                                 1)\n",
    "            loss = self.loss_function(logits, one_hot)\n",
    "            output = (loss,)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def SA_train_epoch(train_loader, model, optimizer, scheduler, epoch, config):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # fgm = FGM(model)\n",
    "    # step number in one epoch: 336\n",
    "    train_losses = 0\n",
    "    for _, batch_samples in enumerate(tqdm(train_loader)):\n",
    "        batch_input, batch_masks, batch_classes = batch_samples\n",
    "        batch_input, batch_masks, batch_classes = batch_input.to(\n",
    "            config.device), batch_masks.to(config.device), batch_classes.to(\n",
    "            config.device)\n",
    "        # compute model output and loss\n",
    "        loss = \\\n",
    "            model(input_ids=batch_input, attention_mask=batch_masks, classes=batch_classes)[0]\n",
    "        train_losses += loss.item()\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        # fgm.attack()\n",
    "        loss_adv = model(input_ids=batch_input, attention_mask=batch_masks, classes=batch_classes)[0]\n",
    "        loss_adv.backward()\n",
    "        # fgm.restore()\n",
    "        # gradient clipping\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config.clip_grad)\n",
    "        # performs updates using calculated gradients\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    train_loss = float(train_losses) / len(train_loader)\n",
    "    print(\"Epoch: {}, train loss: {}\".format(epoch, train_loss))\n",
    "\n",
    "\n",
    "def SA_train(train_loader, eval_dataloader, model, optimizer, scheduler, config):\n",
    "    \"\"\"train the model and test model performance\"\"\"\n",
    "    # reload weights from restore_dir if specified\n",
    "    model.to(config.device)\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    # start training\n",
    "    for epoch in range(1, config.epoch_num + 1):\n",
    "        SA_train_epoch(train_loader, model, optimizer, scheduler, epoch, config)\n",
    "        val_metrics = evaluate(model, eval_dataloader)\n",
    "        val_f1 = val_metrics['f1']\n",
    "        print(\"Epoch: {}, f1 score: {}\".format(epoch, val_f1))\n",
    "        improve_f1 = val_f1 - best_val_f1\n",
    "        if improve_f1 > 1e-5:\n",
    "            best_val_f1 = val_f1\n",
    "            if improve_f1 < config.patience:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        # Early stopping and logging best f1\n",
    "        if (patience_counter >= config.patience_num and epoch > config.min_epoch_num) or epoch == config.epoch_num:\n",
    "            print(\"Best val f1: {}\".format(best_val_f1))\n",
    "            break\n",
    "    print(\"Training Finished!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 计算模型的f1 score。\n",
    "def class_acc(preds, labels):  # preds.shape=(16, 2) labels.shape=torch.Size([16, 1])\n",
    "    # eq里面的两个参数的shape=torch.Size([16])\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()\n",
    "    if 0:\n",
    "        print('binary acc ********')\n",
    "        print('preds = ', preds)\n",
    "        print('labels = ', labels)\n",
    "        print('correct = ', correct)\n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def predict(model, inputs, masks):\n",
    "    model.eval()\n",
    "    inputs, masks = inputs.to(sa_config.device), masks.to(sa_config.device)\n",
    "    batch_output = model(input_ids=inputs, attention_mask=masks, classes=None)[0]\n",
    "    batch_output = torch.max(batch_output, dim=1).indices\n",
    "    return batch_output\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    avg_acc = []\n",
    "    # 表示进入测试模式\n",
    "    model.eval()\n",
    "    device = sa_config.device\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # 从batch中取数据，并放到GPU中\n",
    "            b_input_ids, b_input_mask, b_classes = batch[0].long().to(device), batch[1].long().to(device), batch[\n",
    "                2].long().to(device)\n",
    "            # 前向传播，得到output\n",
    "            output = model(b_input_ids, attention_mask=b_input_mask, classes=None)[0]\n",
    "            # 统计当前batch的acc\n",
    "            acc = class_acc(output, b_classes)\n",
    "            avg_acc.append(acc)\n",
    "    # 统计平均acc\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    print(\"平均acc:{}\".format(avg_acc))\n",
    "    return avg_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 19%|█▉        | 1155/6022 [03:07<13:08,  6.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m eval_dataloader \u001B[38;5;241m=\u001B[39m SA_dataloader(eval_data, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     18\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m get_linear_schedule_with_warmup(optimizer, num_warmup_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     19\u001B[0m                                             num_training_steps\u001B[38;5;241m=\u001B[39msa_config\u001B[38;5;241m.\u001B[39mepoch_num \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader))\n\u001B[1;32m---> 20\u001B[0m \u001B[43mSA_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msa_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mSA_train\u001B[1;34m(train_loader, eval_dataloader, model, optimizer, scheduler, config)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# start training\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, config\u001B[38;5;241m.\u001B[39mepoch_num \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 44\u001B[0m     \u001B[43mSA_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m     val_metrics \u001B[38;5;241m=\u001B[39m evaluate(model, eval_dataloader)\n\u001B[0;32m     46\u001B[0m     val_f1 \u001B[38;5;241m=\u001B[39m val_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mSA_train_epoch\u001B[1;34m(train_loader, model, optimizer, scheduler, epoch, config)\u001B[0m\n\u001B[0;32m     25\u001B[0m     loss_adv\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;66;03m# fgm.restore()\u001B[39;00m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# gradient clipping\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m     \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip_grad_norm_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip_grad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# performs updates using calculated gradients\u001B[39;00m\n\u001B[0;32m     30\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:43\u001B[0m, in \u001B[0;36mclip_grad_norm_\u001B[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001B[0m\n\u001B[0;32m     41\u001B[0m     total_norm \u001B[38;5;241m=\u001B[39m norms[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(norms) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mmax(torch\u001B[38;5;241m.\u001B[39mstack(norms))\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 43\u001B[0m     total_norm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnorm(torch\u001B[38;5;241m.\u001B[39mstack([torch\u001B[38;5;241m.\u001B[39mnorm(g\u001B[38;5;241m.\u001B[39mdetach(), norm_type)\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m grads]), norm_type)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_if_nonfinite \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mlogical_or(total_norm\u001B[38;5;241m.\u001B[39misnan(), total_norm\u001B[38;5;241m.\u001B[39misinf()):\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe total norm of order \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnorm_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for gradients from \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     47\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     48\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     49\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mset `error_if_nonfinite=False`\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:43\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     41\u001B[0m     total_norm \u001B[38;5;241m=\u001B[39m norms[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(norms) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mmax(torch\u001B[38;5;241m.\u001B[39mstack(norms))\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 43\u001B[0m     total_norm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnorm(torch\u001B[38;5;241m.\u001B[39mstack([\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m grads]), norm_type)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_if_nonfinite \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mlogical_or(total_norm\u001B[38;5;241m.\u001B[39misnan(), total_norm\u001B[38;5;241m.\u001B[39misinf()):\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe total norm of order \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnorm_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for gradients from \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     47\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     48\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     49\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mset `error_if_nonfinite=False`\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\Python310\\lib\\site-packages\\torch\\functional.py:1485\u001B[0m, in \u001B[0;36mnorm\u001B[1;34m(input, p, dim, keepdim, out, dtype)\u001B[0m\n\u001B[0;32m   1483\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(p, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1484\u001B[0m         _dim \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(ndim)]  \u001B[38;5;66;03m# noqa: C416 TODO: rewrite as list(range(m))\u001B[39;00m\n\u001B[1;32m-> 1485\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdim\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m   1487\u001B[0m \u001B[38;5;66;03m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001B[39;00m\n\u001B[0;32m   1488\u001B[0m \u001B[38;5;66;03m# remove the overloads where dim is an int and replace with BraodcastingList1\u001B[39;00m\n\u001B[0;32m   1489\u001B[0m \u001B[38;5;66;03m# and remove next four lines, replace _dim with dim\u001B[39;00m\n\u001B[0;32m   1490\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from SA_config import *\n",
    "\n",
    "model = Roberta_FocalLoss(config=sa_config)\n",
    "model.to(sa_config.device)\n",
    "optimizer = AdamW(model.parameters(), lr=sa_config.learning_rate)\n",
    "SA_kfold = StratifiedKFold(n_splits=5).split(train_input_tokens, train_data_class)\n",
    "for train_index, eval_index in SA_kfold:\n",
    "    train_data = TensorDataset(train_input_tokens[train_index], train_attention_tokens[train_index],\n",
    "                               train_data_class[train_index])\n",
    "    eval_data = TensorDataset(train_input_tokens[eval_index], train_attention_tokens[eval_index],\n",
    "                              train_data_class[eval_index])\n",
    "    train_dataloader = SA_dataloader(train_data, True)\n",
    "    eval_dataloader = SA_dataloader(eval_data, False)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                                num_training_steps=sa_config.epoch_num * len(train_dataloader))\n",
    "    SA_train(train_dataloader, eval_dataloader, model, optimizer, scheduler, sa_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['id', 'class'])\n",
    "result['id'] = test_dataframe['id']\n",
    "for index, data in enumerate(tqdm(test_dataloader)):\n",
    "    tokens, masks = data\n",
    "    output = predict(model, tokens, masks)\n",
    "    for b in range(len(output)):\n",
    "        result.loc[index * sa_config.batch_size + b, 'class'] = output[b]\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
