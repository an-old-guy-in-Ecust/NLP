{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P78RFANwg-a9",
    "outputId": "f476ece2-7e26-4cdc-bcdd-c267dae73ff2"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\python310\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in d:\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python310\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: filelock in d:\\python310\\lib\\site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in d:\\python310\\lib\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python310\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python310\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\python310\\lib\\site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in d:\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python310\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\python310\\lib\\site-packages (from requests->transformers) (2.1.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于class类别严重不均衡，先进行数据增强"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "from SA_config import *\n",
    "\n",
    "# sys.path.append('../../EDA_NLP_for_Chinese/code')\n",
    "# from augment import *\n",
    "\n",
    "# 加载训练数据和测试数据\n",
    "# train_dataframe = pd.read_csv('../data_and_baseline/train_data_public.csv', index_col='id')\n",
    "# test_dataframe = pd.read_csv('../data_and_baseline/test_public.csv', index_col='id')\n",
    "# train_dataframe.drop('BIO_anno', inplace=True, axis=1)\n",
    "# # 数据增强\n",
    "# gen_eda(train_dataframe=train_dataframe, alpha=0.1, num_aug=7)\n",
    "# train_dataframe.to_csv('../data_and_baseline/augmented_train_data.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataframe = pd.read_csv('../data_and_baseline/augmented_train_data.csv', index_col='id')\n",
    "test_dataframe = pd.read_csv('../data_and_baseline/test_public.csv', index_col='id')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "access_token = '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'\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext', use_auth_token=access_token)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"NLL loss with label smoothing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        \"\"\"Constructor for the LabelSmoothing module.\n",
    "        :param smoothing: label smoothing factor\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        # 此处的self.smoothing即我们的epsilon平滑参数。\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from SA_config import *\n",
    "\n",
    "\n",
    "class SA_dataloader(DataLoader):\n",
    "    def __init__(self, dataset: TensorDataset, random: bool):\n",
    "        if random:\n",
    "            self.sampler = RandomSampler(dataset)\n",
    "        else:\n",
    "            self.sampler = SequentialSampler(dataset)\n",
    "        super().__init__(dataset, sa_config.batch_size, sampler=self.sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# 将每一句转成数字（大于510做截断，小于510做PADDING，加上首尾两个标识，长度总共等于512）\n",
    "def convert_text_to_token(tokenizer, sentence, limit_size=510):\n",
    "    tokens = tokenizer.encode(sentence[:limit_size])  # 直接截断\n",
    "    # 补齐（pad的索引号就是0）\n",
    "    if len(tokens) < limit_size + 2:\n",
    "        tokens.extend([0] * (limit_size + 2 - len(tokens)))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 建立mask\n",
    "def attention_masks(input_tokens):\n",
    "    atten_masks = []\n",
    "    for seq in input_tokens:\n",
    "        # 如果有编码（>0）即为1, pad为0\n",
    "        seq_mask = [float(x > 0) for x in seq]\n",
    "        atten_masks.append(seq_mask)\n",
    "    return torch.tensor(atten_masks, dtype=torch.bool)\n",
    "\n",
    "\n",
    "# 对每个句子进行编码\n",
    "def input_tokens(tokenizer, data_text_list):\n",
    "    tokens = torch.tensor([convert_text_to_token(tokenizer, x, sa_config.max_position_embeddings - 2) for x in\n",
    "                           data_text_list])\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# train_dataloader: tokens_id,attention-masks,classess\n",
    "train_data_class = torch.tensor(train_dataframe['class'].values)\n",
    "train_data_sentence_list = list(train_dataframe['text'].values)\n",
    "test_data_sentence_list = list(test_dataframe['text'].values)\n",
    "test_input_tokens = input_tokens(tokenizer, test_data_sentence_list)\n",
    "train_input_tokens = input_tokens(tokenizer, train_data_sentence_list)\n",
    "train_attention_tokens = attention_masks(train_input_tokens)\n",
    "test_attention_tokens = attention_masks(test_input_tokens)\n",
    "test_dataset = TensorDataset(test_input_tokens, test_attention_tokens)\n",
    "test_dataloader = SA_dataloader(test_dataset, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "        This criterion is a implemenation of Focal Loss, which is proposed in\n",
    "        Focal Loss for Dense Object Detection.\n",
    "\n",
    "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "\n",
    "        The losses are averaged across observations for each minibatch.\n",
    "\n",
    "        Args:\n",
    "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
    "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5),\n",
    "                                   putting more focus on hard, misclassiﬁed examples\n",
    "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
    "                                However, if the field size_average is set to False, the losses are\n",
    "                                instead summed for each minibatch.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_num=3, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs)\n",
    "\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "        #print(class_mask)\n",
    "\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "\n",
    "        probs = (P * class_mask).sum(1).view(-1, 1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "        #print('probs size= {}'.format(probs.size()))\n",
    "        #print(probs)\n",
    "\n",
    "        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p\n",
    "        #print('-----bacth_loss------')\n",
    "        #print(batch_loss)\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 加入对抗训练\n",
    "class FGM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='bert.embedding.word_embeddings.weight'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='bert.embedding.word_embeddings.weight'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "\n",
    "class Roberta_FocalLoss(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.roberta = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext', config=config)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=config.hidden_size,\n",
    "                                               num_heads=config.num_heads, batch_first=True)\n",
    "        self.GRU = nn.GRU(input_size=config.hidden_size, hidden_size=config.hidden_size // 2, bidirectional=True)\n",
    "        self.classifier = nn.Linear(in_features=config.hidden_size * 2, out_features=config.num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.loss_function = FocalLoss(alpha=config.alpha, class_num=sa_config.num_classes, gamma=5)\n",
    "        # self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, classes=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output, pooler_output = outputs[0], outputs[1]\n",
    "        attention_output, _ = self.attention(output, output, output)\n",
    "        gru_output, _ = self.GRU(attention_output)\n",
    "        concat_output = torch.cat((gru_output[:, -1, :], pooler_output), dim=1)\n",
    "        logits = self.classifier(concat_output)\n",
    "        logits = self.softmax(logits)\n",
    "        output = (logits,)\n",
    "        if classes is not None:\n",
    "            loss = self.loss_function(logits, classes)\n",
    "            output = (loss,)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def SA_train_epoch(train_loader, model, optimizer, scheduler, epoch, config):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    fgm = FGM(model)\n",
    "    # step number in one epoch: 336\n",
    "    train_losses = 0\n",
    "    for _, batch_samples in enumerate(tqdm(train_loader)):\n",
    "        batch_input, batch_masks, batch_classes = batch_samples\n",
    "        batch_input, batch_masks, batch_classes = batch_input.to(\n",
    "            config.device), batch_masks.to(config.device), batch_classes.to(\n",
    "            config.device)\n",
    "        # compute model output and loss\n",
    "        loss = \\\n",
    "            model(input_ids=batch_input, attention_mask=batch_masks, classes=batch_classes)[0]\n",
    "        train_losses += loss.item()\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        fgm.attack()\n",
    "        loss_adv = model(input_ids=batch_input, attention_mask=batch_masks, classes=batch_classes,\n",
    "                         )[0]\n",
    "        loss_adv.backward()\n",
    "        fgm.restore()\n",
    "        # gradient clipping\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config.clip_grad)\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    train_loss = float(train_losses) / len(train_loader)\n",
    "    print(\"Epoch: {}, train loss: {}\".format(epoch, train_loss))\n",
    "\n",
    "\n",
    "def SA_train(train_loader, eval_dataloader, model, optimizer, scheduler, config):\n",
    "    \"\"\"train the model and test model performance\"\"\"\n",
    "    # reload weights from restore_dir if specified\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    # start training\n",
    "    for epoch in range(1, config.epoch_num + 1):\n",
    "        SA_train_epoch(train_loader, model, optimizer, scheduler, epoch, config)\n",
    "        val_acc = evaluate(model, eval_dataloader)\n",
    "        print(\"Epoch: {}, acc score: {}\".format(epoch, val_acc))\n",
    "        improve_acc = val_acc - best_val_acc\n",
    "        if improve_acc > 1e-5:\n",
    "            best_val_acc = val_acc\n",
    "            if improve_acc < config.patience:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        # Early stopping and logging best f1\n",
    "        if (patience_counter >= config.patience_num and epoch > config.min_epoch_num) or epoch == config.epoch_num:\n",
    "            print(\"Best val acc: {}\".format(best_val_acc))\n",
    "            break\n",
    "    print(\"Training Finished!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 计算模型的f1 score。\n",
    "def class_acc(preds, labels):  # preds.shape=(16, 2) labels.shape=torch.Size([16, 1])\n",
    "    # eq里面的两个参数的shape=torch.Size([16])\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()\n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def predict(model, inputs, masks):\n",
    "    model.eval()\n",
    "    inputs, masks = inputs.to(sa_config.device), masks.to(sa_config.device)\n",
    "    batch_output = model(input_ids=inputs, attention_mask=masks, classes=None)[0]\n",
    "    # batch_output = torch.max(batch_output, dim=1).indices.to('cpu')\n",
    "    return batch_output\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    avg_acc = []\n",
    "    # 表示进入测试模式\n",
    "    model.eval()\n",
    "    device = sa_config.device\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # 从batch中取数据，并放到GPU中\n",
    "            b_input_ids, b_input_mask, b_classes = batch[0].long().to(device), batch[1].long().to(device), batch[\n",
    "                2].long().to(device)\n",
    "            # 前向传播，得到output\n",
    "            output = model(b_input_ids, attention_mask=b_input_mask, classes=None)[0]\n",
    "            # 统计当前batch的acc\n",
    "            acc = class_acc(output, b_classes)\n",
    "            avg_acc.append(acc)\n",
    "    # 统计平均acc\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    print(\"平均acc:{}\".format(avg_acc))\n",
    "    return avg_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def linear_combination(x, y, epsilon):\n",
    "    return epsilon * x + (1 - epsilon) * y\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return linear_combination(loss / n, nll, self.epsilon)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from SA_config import *\n",
    "\n",
    "model1 = Roberta_FocalLoss(config=sa_config)\n",
    "model2 = BertForSequenceClassification(config=sa_config)\n",
    "model1.to(sa_config.device)\n",
    "model2.to(sa_config.device)\n",
    "models = [model1, model2]\n",
    "optimizer = AdamW(model1.parameters(), lr=sa_config.learning_rate)\n",
    "SA_kfold = StratifiedKFold(n_splits=5).split(train_input_tokens, train_data_class)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for train_index, eval_index in SA_kfold:\n",
    "    train_data = TensorDataset(train_input_tokens[train_index], train_attention_tokens[train_index],\n",
    "                               train_data_class[train_index])\n",
    "    eval_data = TensorDataset(train_input_tokens[eval_index], train_attention_tokens[eval_index],\n",
    "                              train_data_class[eval_index])\n",
    "    train_dataloader = SA_dataloader(train_data, True)\n",
    "    eval_dataloader = SA_dataloader(eval_data, False)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                                num_training_steps=sa_config.epoch_num * len(train_dataloader))\n",
    "    for model in models:\n",
    "        SA_train(train_dataloader, eval_dataloader, model, optimizer, scheduler, sa_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "result = pd.DataFrame(columns=['id', 'class'])\n",
    "result['id'] = test_dataframe.index\n",
    "for index, data in enumerate(tqdm(test_dataloader)):\n",
    "    tokens, masks = data\n",
    "    # pred = torch.zeros((sa_config.batch_size,sa_config.num)\n",
    "    # for model in models:\n",
    "    output = predict(model1, tokens, masks)\n",
    "    for b in range(len(output)):\n",
    "        result.loc[index * sa_config.batch_size + b, 'class'] = output[b].numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result.to_csv('./SA.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(result['class'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
