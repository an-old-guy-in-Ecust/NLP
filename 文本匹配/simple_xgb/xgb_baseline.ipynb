{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49401, 3) (2000, 3) (2000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99480\\AppData\\Local\\Temp\\ipykernel_28380\\3371921570.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train = pd.read_csv('{}/train.tsv'.format(dataname),\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              query1  \\\n0  1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...   \n1                 1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。   \n2                               还有具体的讨论，公众形象辩论和项目讨论。   \n3                                  当可以保持相当的流速时，结果很高。   \n4                            它是Akmola地区Zerendi区的所在地。   \n\n                                              query2  label  \n0  1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...    0.0  \n1                       1975-76赛季的全国篮球协会是NBA的第30个赛季。    1.0  \n2                                还有公开讨论，特定档案讨论和项目讨论。    0.0  \n3                                 当可以保持可比较的流速时，结果很高。    1.0  \n4                            它是Akmola地区Zerendi区的所在地。    1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query1</th>\n      <th>query2</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...</td>\n      <td>1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。</td>\n      <td>1975-76赛季的全国篮球协会是NBA的第30个赛季。</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>还有具体的讨论，公众形象辩论和项目讨论。</td>\n      <td>还有公开讨论，特定档案讨论和项目讨论。</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>当可以保持相当的流速时，结果很高。</td>\n      <td>当可以保持可比较的流速时，结果很高。</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>它是Akmola地区Zerendi区的所在地。</td>\n      <td>它是Akmola地区Zerendi区的所在地。</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_dataset(dataname):\n",
    "    train = pd.read_csv('{}/train.tsv'.format(dataname),\n",
    "                        sep='[\\t\\f\\r\\v]', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    valid = pd.read_csv('{}/dev.tsv'.format(dataname),\n",
    "                        sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    test = pd.read_csv('{}/test.tsv'.format(dataname),\n",
    "                       sep='\\t', names=['query1', 'query2', 'label'])\n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "\n",
    "name = 'paws-x-zh'\n",
    "train, valid, test = load_dataset(name)\n",
    "print(train.values.shape, valid.values.shape, test.values.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "jieba库的分词原理\n",
    "\n",
    "1. 根据字典统计的每个词的词频，构建前缀树trie树\n",
    "2. 根据当前的句子，统计出现在词典的词构建DAG，利用动态规划进行最大概率路径计算\n",
    "3. 对于OOV的词，采用序列标注，利用HMM模型，viterbi算法进行解码。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [],
   "source": [
    "# 1. 句子A\\B包含的字符个数\n",
    "# 2. 句子A与B的编辑距离\n",
    "# 3. 句子A与B的字符个数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "# import nltk\n",
    "#\n",
    "# nltk.download()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import jieba\n",
    "\n",
    "stops = set(stopwords.words('chinese'))\n",
    "\n",
    "\n",
    "# A、B共有的字符数\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    if row['query1'] is not None:\n",
    "        for word in row['query1']:\n",
    "            if word not in stops:\n",
    "                q1words[word] = 1\n",
    "    if row['query2'] is not None:\n",
    "        for word in row['query2']:\n",
    "            if word not in stops:\n",
    "                q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2)) / (len(q2words) + len(q1words))\n",
    "    return R\n",
    "\n",
    "\n",
    "def token_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    if row['query1'] is not None:\n",
    "        q1 = jieba.lcut(row['query1'])\n",
    "        for word in q1:\n",
    "            if word not in stops:\n",
    "                q1words[word] = 1\n",
    "    if row['query2'] is not None:\n",
    "        q2 = jieba.lcut(row['query2'])\n",
    "        for word in q2:\n",
    "            if word not in stops:\n",
    "                q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2)) / (len(q2words) + len(q1words))\n",
    "    return R"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99480\\AppData\\Local\\Temp\\ipykernel_28380\\778728558.py:43: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  dataset['dist'] = S.fit_transform(dataset['dist'][:, np.newaxis])\n",
      "C:\\Users\\99480\\AppData\\Local\\Temp\\ipykernel_28380\\778728558.py:45: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  dataset['dist'] = S.transform(dataset['dist'][:, np.newaxis])\n",
      "C:\\Users\\99480\\AppData\\Local\\Temp\\ipykernel_28380\\778728558.py:45: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  dataset['dist'] = S.transform(dataset['dist'][:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                  query1  \\\n0      1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...   \n1                     1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。   \n2                                   还有具体的讨论，公众形象辩论和项目讨论。   \n3                                      当可以保持相当的流速时，结果很高。   \n4                                它是Akmola地区Zerendi区的所在地。   \n...                                                  ...   \n49396                  ``我们的学校是精神和精神，热爱（时间路径）是我们的第一承诺''。   \n49397                               她于6月24日在科克，并于7月8日抵达。   \n49398  Cornelia Stuyvesant Vanderbilt（George和Edith Va...   \n49399                      第三季于2010年6月7日首播，第四季是混合情侣竞赛系统。   \n49400             它也位于洛杉矶县大陆海岸的葡萄牙弯曲自然保护区，被称为加州帕洛斯佛得角半岛。   \n\n                                                  query2  label  \\\n0      1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...    0.0   \n1                           1975-76赛季的全国篮球协会是NBA的第30个赛季。    1.0   \n2                                    还有公开讨论，特定档案讨论和项目讨论。    0.0   \n3                                     当可以保持可比较的流速时，结果很高。    1.0   \n4                                它是Akmola地区Zerendi区的所在地。    1.0   \n...                                                  ...    ...   \n49396          ``我们的学校属于时间和精神，对Rehit的爱（精神之路）是我们的第一承诺。 “”    0.0   \n49397                             她于6月24日在科克，并于7月8日抵达唐斯。    1.0   \n49398  John John F. A. Cecil（George和Cornelia Stuyvesa...    0.0   \n49399                 第四季于2010年6月7日首播。就像第三季一样，比赛系统是混合情侣。    0.0   \n49400      它也位于加利福尼亚大陆海岸的一个位置，位于洛杉矶县帕洛斯弗迪斯半岛的葡萄牙本德自然保护区。    0.0   \n\n       characters_of_query1  characters_of_query2  word_match_share  \\\n0                        56                    51          0.666667   \n1                        34                    28          0.904762   \n2                        20                    19          0.560000   \n3                        17                    18          0.956522   \n4                        23                    23          1.000000   \n...                     ...                   ...               ...   \n49396                    33                    41          0.714286   \n49397                    20                    22          0.928571   \n49398                   110                   117          0.949495   \n49399                    29                    34          0.956522   \n49400                    38                    45          0.766667   \n\n       token_match_share                                          tf_idf_q1  \\\n0               0.722222  [0.1963024680302519, 0.1963024680302519, 0.275...   \n1               0.818182  [0.1963024680302519, 0.1963024680302519, 0.196...   \n2               0.615385  [0.36408901109085745, 0.0, 0.36408901109085745...   \n3               1.000000  [0.30184998836473537, 0.0, 0.30184998836473537...   \n4               1.000000  [0.35355339059327373, 0.35355339059327373, 0.3...   \n...                  ...                                                ...   \n49396           0.689655  [0.0, 0.0, 0.20019763520116748, 0.200197635201...   \n49397           0.956522  [0.23570226039551587, 0.23570226039551587, 0.2...   \n49398           0.920000  [0.17414275484853994, 0.17414275484853994, 0.3...   \n49399           0.933333  [0.25019294054773356, 0.25019294054773356, 0.2...   \n49400           0.709677  [0.0, 0.21380087731132585, 0.21380087731132585...   \n\n                                               tf_idf_q2      dist  \n0      [0.16950940407963241, 0.16950940407963241, 0.0... -0.467640  \n1      [0.2295648050491987, 0.2295648050491987, 0.229...  0.574701  \n2      [0.0, 0.3319543885703534, 0.0, 0.2361882814843... -1.095266  \n3      [0.2778778796561673, 0.39054766417182263, 0.27...  0.020937  \n4      [0.35355339059327373, 0.35355339059327373, 0.3...  1.360523  \n...                                                  ...       ...  \n49396  [0.23807087006547265, 0.23807087006547265, 0.1... -0.212755  \n49397  [0.223744822803537, 0.223744822803537, 0.22374...  1.083258  \n49398  [0.15826034850909818, 0.15826034850909818, 0.3...  0.715150  \n49399  [0.21368043965123457, 0.21368043965123457, 0.2... -0.014250  \n49400  [0.2572369802218514, 0.18302623006280597, 0.36... -1.110732  \n\n[49129 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query1</th>\n      <th>query2</th>\n      <th>label</th>\n      <th>characters_of_query1</th>\n      <th>characters_of_query2</th>\n      <th>word_match_share</th>\n      <th>token_match_share</th>\n      <th>tf_idf_q1</th>\n      <th>tf_idf_q2</th>\n      <th>dist</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...</td>\n      <td>1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...</td>\n      <td>0.0</td>\n      <td>56</td>\n      <td>51</td>\n      <td>0.666667</td>\n      <td>0.722222</td>\n      <td>[0.1963024680302519, 0.1963024680302519, 0.275...</td>\n      <td>[0.16950940407963241, 0.16950940407963241, 0.0...</td>\n      <td>-0.467640</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。</td>\n      <td>1975-76赛季的全国篮球协会是NBA的第30个赛季。</td>\n      <td>1.0</td>\n      <td>34</td>\n      <td>28</td>\n      <td>0.904762</td>\n      <td>0.818182</td>\n      <td>[0.1963024680302519, 0.1963024680302519, 0.196...</td>\n      <td>[0.2295648050491987, 0.2295648050491987, 0.229...</td>\n      <td>0.574701</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>还有具体的讨论，公众形象辩论和项目讨论。</td>\n      <td>还有公开讨论，特定档案讨论和项目讨论。</td>\n      <td>0.0</td>\n      <td>20</td>\n      <td>19</td>\n      <td>0.560000</td>\n      <td>0.615385</td>\n      <td>[0.36408901109085745, 0.0, 0.36408901109085745...</td>\n      <td>[0.0, 0.3319543885703534, 0.0, 0.2361882814843...</td>\n      <td>-1.095266</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>当可以保持相当的流速时，结果很高。</td>\n      <td>当可以保持可比较的流速时，结果很高。</td>\n      <td>1.0</td>\n      <td>17</td>\n      <td>18</td>\n      <td>0.956522</td>\n      <td>1.000000</td>\n      <td>[0.30184998836473537, 0.0, 0.30184998836473537...</td>\n      <td>[0.2778778796561673, 0.39054766417182263, 0.27...</td>\n      <td>0.020937</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>它是Akmola地区Zerendi区的所在地。</td>\n      <td>它是Akmola地区Zerendi区的所在地。</td>\n      <td>1.0</td>\n      <td>23</td>\n      <td>23</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>[0.35355339059327373, 0.35355339059327373, 0.3...</td>\n      <td>[0.35355339059327373, 0.35355339059327373, 0.3...</td>\n      <td>1.360523</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49396</th>\n      <td>``我们的学校是精神和精神，热爱（时间路径）是我们的第一承诺''。</td>\n      <td>``我们的学校属于时间和精神，对Rehit的爱（精神之路）是我们的第一承诺。 “”</td>\n      <td>0.0</td>\n      <td>33</td>\n      <td>41</td>\n      <td>0.714286</td>\n      <td>0.689655</td>\n      <td>[0.0, 0.0, 0.20019763520116748, 0.200197635201...</td>\n      <td>[0.23807087006547265, 0.23807087006547265, 0.1...</td>\n      <td>-0.212755</td>\n    </tr>\n    <tr>\n      <th>49397</th>\n      <td>她于6月24日在科克，并于7月8日抵达。</td>\n      <td>她于6月24日在科克，并于7月8日抵达唐斯。</td>\n      <td>1.0</td>\n      <td>20</td>\n      <td>22</td>\n      <td>0.928571</td>\n      <td>0.956522</td>\n      <td>[0.23570226039551587, 0.23570226039551587, 0.2...</td>\n      <td>[0.223744822803537, 0.223744822803537, 0.22374...</td>\n      <td>1.083258</td>\n    </tr>\n    <tr>\n      <th>49398</th>\n      <td>Cornelia Stuyvesant Vanderbilt（George和Edith Va...</td>\n      <td>John John F. A. Cecil（George和Cornelia Stuyvesa...</td>\n      <td>0.0</td>\n      <td>110</td>\n      <td>117</td>\n      <td>0.949495</td>\n      <td>0.920000</td>\n      <td>[0.17414275484853994, 0.17414275484853994, 0.3...</td>\n      <td>[0.15826034850909818, 0.15826034850909818, 0.3...</td>\n      <td>0.715150</td>\n    </tr>\n    <tr>\n      <th>49399</th>\n      <td>第三季于2010年6月7日首播，第四季是混合情侣竞赛系统。</td>\n      <td>第四季于2010年6月7日首播。就像第三季一样，比赛系统是混合情侣。</td>\n      <td>0.0</td>\n      <td>29</td>\n      <td>34</td>\n      <td>0.956522</td>\n      <td>0.933333</td>\n      <td>[0.25019294054773356, 0.25019294054773356, 0.2...</td>\n      <td>[0.21368043965123457, 0.21368043965123457, 0.2...</td>\n      <td>-0.014250</td>\n    </tr>\n    <tr>\n      <th>49400</th>\n      <td>它也位于洛杉矶县大陆海岸的葡萄牙弯曲自然保护区，被称为加州帕洛斯佛得角半岛。</td>\n      <td>它也位于加利福尼亚大陆海岸的一个位置，位于洛杉矶县帕洛斯弗迪斯半岛的葡萄牙本德自然保护区。</td>\n      <td>0.0</td>\n      <td>38</td>\n      <td>45</td>\n      <td>0.766667</td>\n      <td>0.709677</td>\n      <td>[0.0, 0.21380087731132585, 0.21380087731132585...</td>\n      <td>[0.2572369802218514, 0.18302623006280597, 0.36...</td>\n      <td>-1.110732</td>\n    </tr>\n  </tbody>\n</table>\n<p>49129 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "tfidf_model = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "\n",
    "def tf_idf(row):\n",
    "    if row['query1'] is None or row['query2'] is None:\n",
    "        return np.array([0, 0])\n",
    "    q1_words = jieba.lcut(row['query1'])\n",
    "    q2_words = jieba.lcut(row['query2'])\n",
    "    document_1 = ' '.join(q1_words)\n",
    "    document_2 = ' '.join(q2_words)\n",
    "    vector = tfidf_model.fit_transform([document_1, document_2]).toarray()\n",
    "    return np.array(vector)\n",
    "\n",
    "\n",
    "def tf_idf_q1(row):\n",
    "    vector = tf_idf(row)\n",
    "    return vector[0]\n",
    "\n",
    "\n",
    "def tf_idf_q2(row):\n",
    "    vector = tf_idf(row)\n",
    "    return vector[1]\n",
    "\n",
    "\n",
    "S = StandardScaler()\n",
    "for index, dataset in enumerate([train, valid, test]):\n",
    "    if index != 2:\n",
    "        dataset.dropna(axis=0, subset='label', inplace=True)\n",
    "    dataset[\"characters_of_query1\"] = dataset.apply(lambda x: len(x[\"query1\"]) if x['query1'] is not None else 0,\n",
    "                                                    axis=1)  # 有可能为空\n",
    "    dataset[\"characters_of_query2\"] = dataset.apply(lambda x: len(x[\"query2\"]) if x['query2'] is not None else 1,\n",
    "                                                    axis=1)\n",
    "    dataset[\"word_match_share\"] = dataset.apply(word_match_share, axis=1)\n",
    "    dataset[\"token_match_share\"] = dataset.apply(token_match_share, axis=1)\n",
    "    dataset[\"tf_idf_q1\"] = dataset.apply(tf_idf_q1, axis=1)\n",
    "    dataset[\"tf_idf_q2\"] = dataset.apply(tf_idf_q2, axis=1)\n",
    "    dataset['dist'] = dataset.apply(lambda row: np.sum(np.multiply(row['tf_idf_q1'], row['tf_idf_q2'])), axis=1)\n",
    "    if index == 0:\n",
    "        dataset['dist'] = S.fit_transform(dataset['dist'][:, np.newaxis])\n",
    "    else:\n",
    "        dataset['dist'] = S.transform(dataset['dist'][:, np.newaxis])\n",
    "train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "data": {
      "text/plain": "       characters_of_query1  characters_of_query2  word_match_share  \\\n0                        56                    51          0.666667   \n1                        34                    28          0.904762   \n2                        20                    19          0.560000   \n3                        17                    18          0.956522   \n4                        23                    23          1.000000   \n...                     ...                   ...               ...   \n49396                    33                    41          0.714286   \n49397                    20                    22          0.928571   \n49398                   110                   117          0.949495   \n49399                    29                    34          0.956522   \n49400                    38                    45          0.766667   \n\n       token_match_share      dist  \n0               0.722222 -0.467640  \n1               0.818182  0.574701  \n2               0.615385 -1.095266  \n3               1.000000  0.020937  \n4               1.000000  1.360523  \n...                  ...       ...  \n49396           0.689655 -0.212755  \n49397           0.956522  1.083258  \n49398           0.920000  0.715150  \n49399           0.933333 -0.014250  \n49400           0.709677 -1.110732  \n\n[49129 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>characters_of_query1</th>\n      <th>characters_of_query2</th>\n      <th>word_match_share</th>\n      <th>token_match_share</th>\n      <th>dist</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>56</td>\n      <td>51</td>\n      <td>0.666667</td>\n      <td>0.722222</td>\n      <td>-0.467640</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34</td>\n      <td>28</td>\n      <td>0.904762</td>\n      <td>0.818182</td>\n      <td>0.574701</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>19</td>\n      <td>0.560000</td>\n      <td>0.615385</td>\n      <td>-1.095266</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17</td>\n      <td>18</td>\n      <td>0.956522</td>\n      <td>1.000000</td>\n      <td>0.020937</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>23</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.360523</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49396</th>\n      <td>33</td>\n      <td>41</td>\n      <td>0.714286</td>\n      <td>0.689655</td>\n      <td>-0.212755</td>\n    </tr>\n    <tr>\n      <th>49397</th>\n      <td>20</td>\n      <td>22</td>\n      <td>0.928571</td>\n      <td>0.956522</td>\n      <td>1.083258</td>\n    </tr>\n    <tr>\n      <th>49398</th>\n      <td>110</td>\n      <td>117</td>\n      <td>0.949495</td>\n      <td>0.920000</td>\n      <td>0.715150</td>\n    </tr>\n    <tr>\n      <th>49399</th>\n      <td>29</td>\n      <td>34</td>\n      <td>0.956522</td>\n      <td>0.933333</td>\n      <td>-0.014250</td>\n    </tr>\n    <tr>\n      <th>49400</th>\n      <td>38</td>\n      <td>45</td>\n      <td>0.766667</td>\n      <td>0.709677</td>\n      <td>-1.110732</td>\n    </tr>\n  </tbody>\n</table>\n<p>49129 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"characters_of_query1\", \"characters_of_query2\", \"word_match_share\", \"token_match_share\", \"dist\"]\n",
    "train_feature_data = train[columns]\n",
    "train_target_data = train[\"label\"]\n",
    "valid_feature_data = valid[columns]\n",
    "valid_target_data = valid[\"label\"]\n",
    "test_feature_data = test[columns]\n",
    "train_feature_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.69236\ttrain-auc:0.59032\tvalid-logloss:0.69241\tvalid-auc:0.52625\n",
      "[10]\ttrain-logloss:0.68599\ttrain-auc:0.59265\tvalid-logloss:0.68682\tvalid-auc:0.52726\n",
      "[20]\ttrain-logloss:0.68165\ttrain-auc:0.59475\tvalid-logloss:0.68359\tvalid-auc:0.52735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python310\\lib\\site-packages\\xgboost\\core.py:617: FutureWarning: Pass `evals` as keyword args.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\ttrain-logloss:0.67862\ttrain-auc:0.59554\tvalid-logloss:0.68188\tvalid-auc:0.53074\n",
      "[40]\ttrain-logloss:0.67647\ttrain-auc:0.59621\tvalid-logloss:0.68104\tvalid-auc:0.53206\n",
      "[50]\ttrain-logloss:0.67491\ttrain-auc:0.59709\tvalid-logloss:0.68082\tvalid-auc:0.53326\n",
      "[60]\ttrain-logloss:0.67375\ttrain-auc:0.59795\tvalid-logloss:0.68085\tvalid-auc:0.53366\n",
      "[70]\ttrain-logloss:0.67288\ttrain-auc:0.59854\tvalid-logloss:0.68113\tvalid-auc:0.53386\n",
      "[80]\ttrain-logloss:0.67218\ttrain-auc:0.59947\tvalid-logloss:0.68148\tvalid-auc:0.53388\n",
      "[90]\ttrain-logloss:0.67160\ttrain-auc:0.60050\tvalid-logloss:0.68181\tvalid-auc:0.53458\n",
      "[100]\ttrain-logloss:0.67114\ttrain-auc:0.60125\tvalid-logloss:0.68230\tvalid-auc:0.53469\n",
      "[110]\ttrain-logloss:0.67075\ttrain-auc:0.60198\tvalid-logloss:0.68272\tvalid-auc:0.53514\n",
      "[120]\ttrain-logloss:0.67037\ttrain-auc:0.60286\tvalid-logloss:0.68312\tvalid-auc:0.53531\n",
      "[130]\ttrain-logloss:0.67005\ttrain-auc:0.60367\tvalid-logloss:0.68346\tvalid-auc:0.53542\n",
      "[140]\ttrain-logloss:0.66978\ttrain-auc:0.60446\tvalid-logloss:0.68370\tvalid-auc:0.53564\n",
      "[150]\ttrain-logloss:0.66953\ttrain-auc:0.60509\tvalid-logloss:0.68395\tvalid-auc:0.53544\n",
      "[160]\ttrain-logloss:0.66930\ttrain-auc:0.60563\tvalid-logloss:0.68418\tvalid-auc:0.53583\n",
      "[170]\ttrain-logloss:0.66909\ttrain-auc:0.60617\tvalid-logloss:0.68427\tvalid-auc:0.53612\n",
      "[180]\ttrain-logloss:0.66888\ttrain-auc:0.60668\tvalid-logloss:0.68445\tvalid-auc:0.53605\n",
      "[190]\ttrain-logloss:0.66869\ttrain-auc:0.60714\tvalid-logloss:0.68455\tvalid-auc:0.53643\n",
      "[200]\ttrain-logloss:0.66851\ttrain-auc:0.60751\tvalid-logloss:0.68469\tvalid-auc:0.53665\n",
      "[210]\ttrain-logloss:0.66835\ttrain-auc:0.60783\tvalid-logloss:0.68489\tvalid-auc:0.53646\n",
      "[220]\ttrain-logloss:0.66821\ttrain-auc:0.60813\tvalid-logloss:0.68497\tvalid-auc:0.53689\n",
      "[230]\ttrain-logloss:0.66805\ttrain-auc:0.60851\tvalid-logloss:0.68501\tvalid-auc:0.53692\n",
      "[240]\ttrain-logloss:0.66791\ttrain-auc:0.60881\tvalid-logloss:0.68522\tvalid-auc:0.53688\n",
      "[250]\ttrain-logloss:0.66777\ttrain-auc:0.60909\tvalid-logloss:0.68525\tvalid-auc:0.53748\n",
      "[260]\ttrain-logloss:0.66764\ttrain-auc:0.60939\tvalid-logloss:0.68527\tvalid-auc:0.53797\n",
      "[270]\ttrain-logloss:0.66752\ttrain-auc:0.60964\tvalid-logloss:0.68529\tvalid-auc:0.53816\n",
      "[280]\ttrain-logloss:0.66741\ttrain-auc:0.60994\tvalid-logloss:0.68522\tvalid-auc:0.53895\n",
      "[290]\ttrain-logloss:0.66728\ttrain-auc:0.61037\tvalid-logloss:0.68511\tvalid-auc:0.53985\n",
      "[300]\ttrain-logloss:0.66713\ttrain-auc:0.61078\tvalid-logloss:0.68512\tvalid-auc:0.54034\n",
      "[310]\ttrain-logloss:0.66702\ttrain-auc:0.61111\tvalid-logloss:0.68497\tvalid-auc:0.54103\n",
      "[320]\ttrain-logloss:0.66685\ttrain-auc:0.61161\tvalid-logloss:0.68509\tvalid-auc:0.54090\n",
      "[330]\ttrain-logloss:0.66668\ttrain-auc:0.61208\tvalid-logloss:0.68510\tvalid-auc:0.54106\n",
      "[340]\ttrain-logloss:0.66654\ttrain-auc:0.61246\tvalid-logloss:0.68522\tvalid-auc:0.54099\n",
      "[350]\ttrain-logloss:0.66639\ttrain-auc:0.61284\tvalid-logloss:0.68534\tvalid-auc:0.54060\n",
      "[360]\ttrain-logloss:0.66630\ttrain-auc:0.61310\tvalid-logloss:0.68537\tvalid-auc:0.54026\n",
      "[370]\ttrain-logloss:0.66614\ttrain-auc:0.61357\tvalid-logloss:0.68546\tvalid-auc:0.54000\n",
      "[380]\ttrain-logloss:0.66600\ttrain-auc:0.61399\tvalid-logloss:0.68562\tvalid-auc:0.53953\n",
      "[388]\ttrain-logloss:0.66589\ttrain-auc:0.61426\tvalid-logloss:0.68562\tvalid-auc:0.53969\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "d_train_data = xgb.DMatrix(train_feature_data.values, label=train_target_data.values)\n",
    "d_eval_data = xgb.DMatrix(valid_feature_data.values, label=valid_target_data.values)\n",
    "params = {'max_depth': 4, 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'auc'], 'eta': 0.02}\n",
    "watchlist = [(d_train_data, 'train'), (d_eval_data, 'valid')]\n",
    "bst = xgb.train(params, d_train_data, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "d_test_data = xgb.DMatrix(test_feature_data)\n",
    "predict_test_data = bst.predict(d_test_data)\n",
    "predict_test_data = [0 if data < 0.5 else 1 for data in predict_test_data]\n",
    "sub = pd.DataFrame(columns=['index', 'prediction'])\n",
    "sub['index'] = test_feature_data.index\n",
    "sub['prediction'] = predict_test_data\n",
    "sub.to_csv('{}.tsv'.format(name), index=False, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [
    {
     "data": {
      "text/plain": "      index  prediction\n0         0           0\n1         1           0\n2         2           0\n3         3           0\n4         4           0\n...     ...         ...\n1995   1995           0\n1996   1996           0\n1997   1997           1\n1998   1998           0\n1999   1999           0\n\n[2000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>1995</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>1996</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>1997</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>1998</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>1999</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
